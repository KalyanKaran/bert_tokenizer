# ğŸš€ IMDb Sentiment Classification with BERT (CPU-Optimized)

This project fine-tunes the **BERT (bert-base-uncased)** model on the **IMDb movie review dataset** to perform **binary sentiment classification** (positive/negative). It is designed to run efficiently on **CPU** inside a **Dockerized environment**.

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ train.py           # Training script
â”œâ”€â”€ Dockerfile              # Docker setup
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md               # Documentation (this file)
â””â”€â”€ results/                # Saved fine-tuned model will be genarated here (generated after training)

---

## ğŸ“Œ Features

âœ… Fine-tunes **bert-base-uncased** on IMDb movie reviews  
âœ… Uses **Hugging Face Transformers** for training  
âœ… **Optimized for CPU** (small batch size, `no_cuda=True`)  
âœ… **Dockerized** for easy setup and portability  
âœ… Logs training progress every **100 steps**  
âœ… Evaluates the model and performs a **sample prediction** after training  

---

## ğŸš€ Setup Instructions

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/KalyanKaran/bert_tokenizer.git
cd bert_tokenizer

2ï¸âƒ£ Build the Docker Image

docker build -t bert_tokenizer .

3ï¸âƒ£ Run the Training Process

docker run --rm bert_tokenizer

ğŸ¯ Expected Output
	â€¢	Training progress (loss, accuracy) printed every 100 steps
	â€¢	Final evaluation results on the IMDb test dataset
	â€¢	A sample sentiment prediction after training

Example Output:

ğŸš€ Training started on CPU...
âœ… Evaluation Results: {'eval_loss': 0.30, 'eval_accuracy': 0.89}
ğŸ“ Sample Text: This movie was absolutely fantastic!
ğŸ”® Predicted Sentiment: Positive
âœ… Model fine-tuning completed and saved!

ğŸ“Š Model Evaluation

After training, the model is saved inside the fine_tuned_model/ directory.

To manually test it, run:

docker run --rm -it my-ai-ml-project /bin/sh
python -c "from transformers import AutoModelForSequenceClassification, AutoTokenizer; model = AutoModelForSequenceClassification.from_pretrained('./fine_tuned_model'); tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_model'); text = 'This movie was amazing!'; inputs = tokenizer(text, return_tensors='pt'); print(model(**inputs).logits)"

âš™ï¸ Modifications & Customization
	â€¢	To increase accuracy, modify train.py to:
	â€¢	Increase epochs (num_train_epochs=3 â†’ 5)
	â€¢	Adjust learning rate (2e-5 â†’ 3e-5)
	â€¢	Use larger batch size if you switch to GPU
	â€¢	To use a different dataset, replace:

dataset = load_dataset("imdb")

with:

dataset = load_dataset("csv", data_files={"train": "data/train.csv", "validation": "data/val.csv"})

and provide your own dataset in data/.

ğŸ“Œ Requirements

âœ” Docker
âœ” Python 3.9+
âœ” Hugging Face transformers, datasets, torch (installed via requirements.txt)

ğŸ“œ License

This project is MIT Licensed â€“ you are free to use, modify, and distribute it.

ğŸ’¡ Credits
	â€¢	Built using Hugging Face Transformers ğŸš€
	â€¢	IMDb dataset from Hugging Face Datasets
	â€¢	Model: BERT (bert-base-uncased)

â“ Need Help?

ğŸ’¬ Reach out via Issues or Discussions!

ğŸš€ Happy Fine-Tuning!

This **README.md** is now in a **single file** while keeping it detailed and well-structured. ğŸš€ğŸ”¥ Let me know if you need any changes!
